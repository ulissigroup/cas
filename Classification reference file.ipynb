{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRand(r1, r2, num):\n",
    "    return (r1 - r2) * torch.rand(num, 1) + r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(num_data):\n",
    "    x = getRand(-3, 3, num_data)\n",
    "    y = getRand(-3, 3, num_data)\n",
    "    data_fn = lambda x, y: torch.sin(0.15 * 3.1415 * (x + y)) + 1\n",
    "    latent_fn = data_fn(x, y)\n",
    "    z = latent_fn.long().squeeze()  #This will convert z to integers\n",
    "    return torch.cat((x,y),dim=1), z, data_fn\n",
    "\n",
    "train_x, train_y, genfn = gen_data(20)\n",
    "\n",
    "plt.scatter(train_x[:,0].numpy(), train_x[:,1].numpy(), c = train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d1 = np.linspace(-3, 3, 150)\n",
    "test_d2 = np.linspace(-3, 3, 150)\n",
    "\n",
    "test_x_mat, test_y_mat = np.meshgrid(test_d1, test_d2)\n",
    "test_x_mat, test_y_mat = torch.Tensor(test_x_mat), torch.Tensor(test_y_mat)\n",
    "\n",
    "test_x = torch.cat((test_x_mat.view(-1,1), test_y_mat.view(-1,1)),dim=1)\n",
    "\n",
    "test_labels = genfn(test_x_mat, test_y_mat).long().squeeze() #Convert to integers (0 or 1)\n",
    "\n",
    "test_y = test_labels.view(-1)\n",
    "\n",
    "plt.contourf(test_x_mat.numpy(), test_y_mat.numpy(), test_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpytorch.models import ExactGP\n",
    "# from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "# from gpytorch.means import ConstantMean\n",
    "# from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "# class DirichletGPModel(ExactGP):\n",
    "#     def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "#         super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "#         self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "#         self.covar_module = ScaleKernel(\n",
    "#             RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "#             batch_shape=torch.Size((num_classes,)),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Botorch stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel\n",
    "from botorch.models.utils import multioutput_to_batch_mode_transform\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from botorch.acquisition.objective import IdentityMCObjective\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import ModelListGP, SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.sampling import sample_hypersphere\n",
    "from botorch.utils.transforms import t_batch_mode_transform\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch.quasirandom import SobolEngine\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import random\n",
    "\n",
    "class DirichletGPModel(ExactGP, BatchedMultiOutputGPyTorchModel):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes, input_transform=True):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        #-----------------------------------------------------------------------#\n",
    "\n",
    "        self._set_dimensions(train_X=train_x, train_Y=train_y)\n",
    "        train_X, train_Y, _ = multioutput_to_batch_mode_transform(\n",
    "            train_X=train_x, train_Y=train_y, num_outputs=self._num_outputs\n",
    "        )\n",
    "\n",
    "        #-----------------------------------------------------------------------#\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECI stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask(x, a, eps=2e-3):\n",
    "    \"\"\"Returns 0ish for x < a and 1ish for x > a\"\"\"\n",
    "    return torch.nn.Sigmoid()((x - a) / eps)\n",
    "\n",
    "\n",
    "def smooth_box_mask(x, a, b, eps=2e-3):\n",
    "    \"\"\"Returns 1ish for a < x < b and 0ish otherwise\"\"\"\n",
    "    return smooth_mask(x, a, eps) - smooth_mask(x, b, eps)\n",
    "\n",
    "class ExpectedCoverageImprovement(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        constraints,\n",
    "        punchout_radius,\n",
    "        bounds,\n",
    "        num_samples=512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Expected Coverage Improvement (q=1 required, analytic)\n",
    "\n",
    "        Right now, we assume that all the models in the ModelListGP have\n",
    "        the same training inputs.\n",
    "\n",
    "        Args:\n",
    "            model: A ModelListGP object containing models matching the corresponding constraints.\n",
    "                All models are assumed to have the same training data.\n",
    "            constraints: List containing 2-tuples with (direction, value), e.g.,\n",
    "                [('gt', 3), ('lt', 4)]. It is necessary that\n",
    "                len(constraints) == model.num_outputs.\n",
    "            punchout_radius: Positive value defining the desired minimum distance between points\n",
    "            bounds: torch.tensor whose first row is the lower bounds and second row is the upper bounds\n",
    "            num_samples: Number of samples for MC integration\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, objective=IdentityMCObjective(), **kwargs)\n",
    "        assert len(constraints) == model.num_outputs\n",
    "        assert all(direction in (\"gt\", \"lt\") for direction, _ in constraints)\n",
    "        assert punchout_radius > 0\n",
    "        self.constraints = constraints\n",
    "        self.punchout_radius = punchout_radius\n",
    "        self.bounds = bounds\n",
    "        self.base_points = self.train_inputs\n",
    "        self.ball_of_points = self._generate_ball_of_points(\n",
    "            num_samples=num_samples,\n",
    "            radius=punchout_radius,\n",
    "            device=bounds.device,\n",
    "            dtype=bounds.dtype,\n",
    "        )\n",
    "        self._thresholds = torch.tensor(\n",
    "            [threshold for _, threshold in self.constraints]\n",
    "        ).to(bounds)\n",
    "        assert (\n",
    "            all(ub > lb for lb, ub in self.bounds.T) and len(self.bounds.T) == self.dim\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_outputs(self):\n",
    "        return self.model.num_outputs\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.train_inputs.shape[-1]\n",
    "\n",
    "    @property\n",
    "    def train_inputs(self):\n",
    "        return self.model.models[0].train_inputs[0]\n",
    "\n",
    "    def _generate_ball_of_points(\n",
    "        self, num_samples, radius, device=None, dtype=torch.double\n",
    "    ):\n",
    "        \"\"\"Creates a ball of points to be used for MC.\"\"\"\n",
    "        tkwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        z = sample_hypersphere(d=self.dim, n=num_samples, qmc=True, **tkwargs)\n",
    "        r = torch.rand(num_samples, 1, **tkwargs) ** (1 / self.dim)\n",
    "        return radius * r * z\n",
    "\n",
    "    def _get_base_point_mask(self, X):\n",
    "        distance_matrix = self.model.models[0].covar_module.base_kernel.covar_dist(\n",
    "            X, self.base_points\n",
    "        )\n",
    "        return smooth_mask(distance_matrix, self.punchout_radius)\n",
    "\n",
    "    def _estimate_probabilities_of_satisfaction_at_points(self, points):\n",
    "        \"\"\"Estimate the probability of satisfying the given constraints.\"\"\"\n",
    "        posterior = self.model.posterior(X=points)\n",
    "        mus, sigma2s = posterior.mean, posterior.variance\n",
    "        dist = torch.distributions.normal.Normal(mus, sigma2s.sqrt())\n",
    "        norm_cdf = dist.cdf(self._thresholds)\n",
    "        probs = torch.ones(points.shape[:-1]).to(points)\n",
    "        for i, (direction, _) in enumerate(self.constraints):\n",
    "            probs = probs * (\n",
    "                norm_cdf[..., i] if direction == \"lt\" else 1 - norm_cdf[..., i]\n",
    "            )\n",
    "        return probs\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X):\n",
    "        \"\"\"Evaluate Expected Improvement on the candidate set X.\"\"\"\n",
    "        ball_around_X = self.ball_of_points + X\n",
    "        domain_mask = smooth_mask(\n",
    "            ball_around_X, self.bounds[0, :], self.bounds[1, :]\n",
    "        ).prod(dim=-1)\n",
    "        num_points_in_integral = domain_mask.sum(dim=-1)\n",
    "        base_point_mask = self._get_base_point_mask(ball_around_X).prod(dim=-1)\n",
    "        prob = self._estimate_probabilities_of_satisfaction_at_points(ball_around_X)\n",
    "        masked_prob = prob * domain_mask * base_point_mask\n",
    "        y = masked_prob.sum(dim=-1) / num_points_in_integral\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = DirichletClassificationLikelihood(train_y, learn_additional_noise=True)\n",
    "model = DirichletGPModel(train_x, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "# fit_gpytorch_model(mll)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "    loss.backward()\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.mean().item(),\n",
    "            model.likelihood.second_noise_covar.noise.mean().item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "    test_dist = model(test_x)\n",
    "    pred_means = test_dist.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "for i in range(2):\n",
    "    im = ax[i].contourf(\n",
    "        test_x_mat.numpy(), test_y_mat.numpy(), pred_means[i].numpy().reshape((150,150))\n",
    "    )\n",
    "    fig.colorbar(im, ax=ax[i])\n",
    "    ax[i].set_title(\"Logits: Class \" + str(i), fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_samples = test_dist.sample(torch.Size((256,))).exp()\n",
    "probabilities = (pred_samples / pred_samples.sum(-2, keepdim=True)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "levels = np.linspace(0, 1.05, 20)\n",
    "for i in range(2):\n",
    "    im = ax[i].contourf(\n",
    "        test_x_mat.numpy(), test_y_mat.numpy(), probabilities[i].numpy().reshape((150,150)), levels=levels\n",
    "    )\n",
    "    fig.colorbar(im, ax=ax[i])\n",
    "    ax[i].set_title(\"Probabilities: Class \" + str(i), fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "\n",
    "ax[0].contourf(test_x_mat.numpy(), test_y_mat.numpy(), test_labels.numpy())\n",
    "ax[0].set_title('True Response', fontsize=20)\n",
    "\n",
    "ax[1].contourf(test_x_mat.numpy(), test_y_mat.numpy(), pred_means.max(0)[1].reshape((150,150)))\n",
    "ax[1].set_title('Estimated Response', fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fa163922eb0b3709bbb5d8082b2465c9de796dbaacca80cbaa600e7fff3e4fe"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
