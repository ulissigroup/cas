{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd180690",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gpytorch\n",
    "from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel\n",
    "from botorch.models.utils import multioutput_to_batch_mode_transform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from botorch.acquisition.objective import IdentityMCObjective\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import ModelListGP, SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.sampling import sample_hypersphere\n",
    "from botorch.utils.transforms import t_batch_mode_transform\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch.quasirandom import SobolEngine\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import random\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
    "# def f(x1, x2): return np.sin(x1+x2)+(x1-x2)**2-1.5*x1+2.5*x2+1\n",
    "def f(x1, x2): return (x1**2+x2-11)**2+(x1+x2**2-7)**2\n",
    "x1 = np.linspace(-3, 3)\n",
    "x2 = np.linspace(-3, 3)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "# F = f(x1, x2) + random.random()\n",
    "plt.contour(X1, X2, f(X1, X2)+random.random(), cmap = 'viridis')\n",
    "cbar = plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b07131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkwargs = {\n",
    "    # \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"device\": torch.device(\"cpu\"),\n",
    "    \"dtype\": torch.double,\n",
    "    # \"dtype\": torch.float,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eacaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask(x, a, eps=2e-3):\n",
    "    \"\"\"Returns 0ish for x < a and 1ish for x > a\"\"\"\n",
    "    return torch.nn.Sigmoid()((x - a) / eps)\n",
    "\n",
    "\n",
    "def smooth_box_mask(x, a, b, eps=2e-3):\n",
    "    \"\"\"Returns 1ish for a < x < b and 0ish otherwise\"\"\"\n",
    "    return smooth_mask(x, a, eps) - smooth_mask(x, b, eps)\n",
    "\n",
    "class ExpectedCoverageImprovement(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        constraints,\n",
    "        punchout_radius,\n",
    "        bounds,\n",
    "        num_samples=512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Expected Coverage Improvement (q=1 required, analytic)\n",
    "\n",
    "        Right now, we assume that all the models in the ModelListGP have\n",
    "        the same training inputs.\n",
    "\n",
    "        Args:\n",
    "            model: A ModelListGP object containing models matching the corresponding constraints.\n",
    "                All models are assumed to have the same training data.\n",
    "            constraints: List containing 2-tuples with (direction, value), e.g.,\n",
    "                [('gt', 3), ('lt', 4)]. It is necessary that\n",
    "                len(constraints) == model.num_outputs.\n",
    "            punchout_radius: Positive value defining the desired minimum distance between points\n",
    "            bounds: torch.tensor whose first row is the lower bounds and second row is the upper bounds\n",
    "            num_samples: Number of samples for MC integration\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, objective=IdentityMCObjective(), **kwargs)\n",
    "        assert len(constraints) == model.num_outputs\n",
    "        assert all(direction in (\"gt\", \"lt\") for direction, _ in constraints)\n",
    "        assert punchout_radius > 0\n",
    "        self.constraints = constraints\n",
    "        self.punchout_radius = punchout_radius\n",
    "        self.bounds = bounds\n",
    "        self.base_points = self.train_inputs\n",
    "        self.ball_of_points = self._generate_ball_of_points(\n",
    "            num_samples=num_samples,\n",
    "            radius=punchout_radius,\n",
    "            device=bounds.device,\n",
    "            dtype=bounds.dtype,\n",
    "        )\n",
    "        self._thresholds = torch.tensor(\n",
    "            [threshold for _, threshold in self.constraints]\n",
    "        ).to(bounds)\n",
    "        assert (\n",
    "            all(ub > lb for lb, ub in self.bounds.T) and len(self.bounds.T) == self.dim\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_outputs(self):\n",
    "        return self.model.num_outputs\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.train_inputs.shape[-1]\n",
    "\n",
    "    @property\n",
    "    def train_inputs(self):\n",
    "        return self.model.models[0].train_inputs[0]\n",
    "\n",
    "    def _generate_ball_of_points(\n",
    "        self, num_samples, radius, device=None, dtype=torch.double\n",
    "    ):\n",
    "        \"\"\"Creates a ball of points to be used for MC.\"\"\"\n",
    "        tkwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        z = sample_hypersphere(d=self.dim, n=num_samples, qmc=True, **tkwargs) # Not using self.dim\n",
    "        r = torch.rand(num_samples, 1, **tkwargs) ** (1 / self.dim)\n",
    "        \n",
    "        return radius * r * z\n",
    "\n",
    "    def _get_base_point_mask(self, X):\n",
    "        distance_matrix = self.model.models[0].covar_module.base_kernel.covar_dist(\n",
    "            X, self.base_points.double()\n",
    "        )   # Note to self: self.base_points is fp32\n",
    "            # Should standardize all to fp64?\n",
    "        return smooth_mask(distance_matrix, self.punchout_radius)\n",
    "\n",
    "    def _estimate_probabilities_of_satisfaction_at_points(self, points):\n",
    "        print(\"Entered estimate prob\")\n",
    "        probabilities = torch.zeros((points.shape[0:2]))\n",
    "        for i in range(len(points)):\n",
    "            with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "                # print(\"Before test dist\")\n",
    "                # print(f\"self.model::::::: {self.model.posterior()[0]}\")\n",
    "                # test_dist = self.model.posterior(points)\n",
    "                test_dist = model(points[i].float()) # Calculate posterior\n",
    "                # print(\"Before pred_means\")\n",
    "                pred_means = test_dist.loc\n",
    "            pred_samples = test_dist.sample(torch.Size((50,))).exp()\n",
    "            prob_of_one_point = (pred_samples / pred_samples.sum(-2, keepdim=True))[:,1,:].mean(0)\n",
    "            probabilities[i] = prob_of_one_point\n",
    "        return probabilities\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X):\n",
    "        \"\"\"Evaluate Expected Improvement on the candidate set X.\"\"\"\n",
    "        ball_around_X = self.ball_of_points + X\n",
    "        domain_mask = smooth_mask(\n",
    "            ball_around_X, self.bounds[0, :], self.bounds[1, :]\n",
    "        ).prod(dim=-1)\n",
    "        # print(f\"domain mask: {domain_mask.shape}\")\n",
    "        num_points_in_integral = domain_mask.sum(dim=-1)\n",
    "        # print(f\"num_points_in_integral: {num_points_in_integral.shape}\")\n",
    "        # print(\"Right before base_point_mask\")\n",
    "        # print(f\"ball dtype: {ball_around_X.dtype}\")\n",
    "        # print(f\"ball shape: {ball_around_X.shape}\")\n",
    "        base_point_mask = self._get_base_point_mask(ball_around_X).prod(dim=-1)\n",
    "        # print(f\"base_point_mask: {base_point_mask.shape}\")\n",
    "        prob = self._estimate_probabilities_of_satisfaction_at_points(ball_around_X)\n",
    "        # print(f\"prob: {prob.shape}\")\n",
    "        masked_prob = prob * domain_mask * base_point_mask\n",
    "        y = masked_prob.sum(dim=-1) / num_points_in_integral\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c157cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpytorch.models import ExactGP\n",
    "# from botorch.models.gpytorch import GPyTorchModel\n",
    "# from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "# from gpytorch.means import ConstantMean\n",
    "# from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "# # We will use the simplest form of GP model, exact inference\n",
    "# class DirichletGPModel(ExactGP, GPyTorchModel):\n",
    "#     def __init__(self, train_x, train_y, likelihood, num_classes, input_transform=True):\n",
    "#         super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "#         self.mean_module = ConstantMean()\n",
    "#         self.covar_module = ScaleKernel(RBFKernel())\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_fit_gp(X, Y):\n",
    "    # Find optimal model hyperparameters\n",
    "    X = X.float()\n",
    "    likelihood = DirichletClassificationLikelihood(Y[:,0].long(), learn_additional_noise=True)\n",
    "    model = DirichletGPModel(X, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    '''\n",
    "    Temporary hack (X.float())\n",
    "    need to fix dtype later\n",
    "    '''\n",
    "\n",
    "    for i in range(50):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        # model.eval()\n",
    "        # likelihood.eval()\n",
    "        output = model(X)\n",
    "        # Calc loss and backprop gradients\n",
    "        # model.train()\n",
    "        # likelihood.train()\n",
    "        loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "        loss.backward()\n",
    "        # if (i+1) % 5 == 0:\n",
    "        #     print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        #         i + 1, 50, loss.item(),\n",
    "        #         model.covar_module.base_kernel.lengthscale.mean().item(),\n",
    "        #         model.likelihood.second_noise_covar.noise.mean().item()\n",
    "        #     ))\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yf(x):\n",
    "    v = (x[:,0]**2+x[:,1]-11)**2+(x[:,0]+x[:,1]**2-7)**2\n",
    "    for i in range(len(v)):\n",
    "        if v[i] > 160:\n",
    "            v[i] = 1\n",
    "        else:\n",
    "            v[i] = 0\n",
    "    return torch.stack((v, v), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b67e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = torch.tensor([[-3, -3], [3, 3]], **tkwargs)\n",
    "lb, ub = bounds\n",
    "dim = len(lb)\n",
    "punchout_radius = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd6612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_init_points = 5\n",
    "# num_total_points = 20 \n",
    "# X = lb + (ub - lb) * SobolEngine(dim, scramble=True).draw(num_init_points).to(**tkwargs)\n",
    "# Y = yf(X)\n",
    "# plt.scatter(X.cpu().numpy()[:, 0], X.cpu().numpy()[:, 1], c=Y.cpu()[:,0])\n",
    "\n",
    "\n",
    "num_init_points = 10\n",
    "num_total_points = 15 \n",
    "# X = lb + (ub - lb) * SobolEngine(dim, scramble=True).draw(num_init_points).to(**tkwargs)\n",
    "def get_first_N_points(num):\n",
    "    with open(\"./trainx.txt\",\"r\") as x:\n",
    "        data = eval(x.read())\n",
    "    train_x = torch.tensor(data)[0:num,:]\n",
    "    with open(\"./trainy.txt\",\"r\") as y:\n",
    "        data = eval(y.read())\n",
    "    train_y = torch.tensor(data)[0:num]\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "X, Y = get_first_N_points(10)\n",
    "\n",
    "X = X.double()\n",
    "Y = Y.unsqueeze(-1).repeat(1,2).double()\n",
    "# print(Y)\n",
    "# print(Y.shape)\n",
    "# Y = yf(X)\n",
    "plt.scatter(X.numpy()[:, 0], X.numpy()[:, 1], c=Y[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a091b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class DirichletGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "# we let the DirichletClassificationLikelihood compute the targets for us\n",
    "X = X.float()\n",
    "likelihood = DirichletClassificationLikelihood(Y[:,0].long(), learn_additional_noise=True)\n",
    "model = DirichletGPModel(X, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [(\"lt\", 1.01), (\"gt\", 0.95)]\n",
    "# normalization\n",
    "# mean = X.mean(dim=-2, keepdim=True)\n",
    "# std = X.std(dim=-2, keepdim=True) + 1e-6 # prevent dividing by 0\n",
    "# X = (X - mean) / std\n",
    "# testnum = 1\n",
    "while len(X) < num_total_points:\n",
    "    # We don't have to normalize X since the domain is [0, 1]^2. Make sure to\n",
    "    # appropriately adjust the punchout radius if the domain is normalized.\n",
    "    print(\"Checkpoint: Calling get_and_fit_gp\")\n",
    "    # gp_models = [get_and_fit_gp(X, Y[:, 0 : 1])]\n",
    "    gp_models = get_and_fit_gp(X.float(), Y[:, 0: 1])\n",
    "    print(\"Checkpoint: Calling ModelListGP\")\n",
    "    model_list_gp = ModelListGP(gp_models, gp_models) # Temporary hack\n",
    "    print(\"Checkpoint: ECI\")\n",
    "    eci = ExpectedCoverageImprovement(\n",
    "        model=model_list_gp,\n",
    "        constraints=constraints,\n",
    "        punchout_radius=punchout_radius,\n",
    "        bounds=bounds,\n",
    "        num_samples=512,\n",
    "    )\n",
    "    print(\"Checkpoint: x_next\")\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    x_next, _ = optimize_acqf(\n",
    "        acq_function=eci,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=10,\n",
    "        raw_samples=512,\n",
    "    )\n",
    "    print(f\"Got x_next: {x_next}\")\n",
    "    # x_next = torch.tensor([[testnum,testnum+0.1]])\n",
    "    # testnum += 1\n",
    "    print(\"Checkpoint: y_next\")\n",
    "    y_next = yf(x_next)\n",
    "    # print(f\"x_next dtype:{x_next.dtype}\")\n",
    "    # print(f\"x_next shape:{x_next.shape}\")\n",
    "    print(\"Checkpoint: finished y_next\")\n",
    "    # print(f\"X shape:{X.shape}\")\n",
    "    # print(f\"Y shape:{Y.shape}\")\n",
    "    # print(X)\n",
    "    X = torch.cat((X, x_next))\n",
    "    Y = torch.cat((Y, y_next))\n",
    "    print(\"After cat\")\n",
    "    # print(X)\n",
    "    # print(f\"X shape:{X.shape}\")\n",
    "    # print(f\"Y shape:{Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579bad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N1, N2 = 50, 50\n",
    "Xplt, Yplt = torch.meshgrid(\n",
    "    torch.linspace(-3, 3, N1, **tkwargs), torch.linspace(-3, 3, N2, **tkwargs)\n",
    ")\n",
    "xplt = torch.stack(\n",
    "    (\n",
    "        torch.reshape(Xplt, (Xplt.shape[0] * Xplt.shape[1],)),\n",
    "        torch.reshape(Yplt, (Yplt.shape[0] * Yplt.shape[1],)),\n",
    "    ),\n",
    "    dim=1,\n",
    ")\n",
    "yplt = yf(xplt)\n",
    "Zplt = torch.reshape(yplt[:, 0], (N1, N2)) \n",
    "def identify_samples_which_satisfy_constraints(X, constraints):\n",
    "    \"\"\"\n",
    "    Takes in values (a1, ..., ak, o) and returns (a1, ..., ak, o)\n",
    "    True/False values, where o is the number of outputs.\n",
    "    \"\"\"\n",
    "    successful = torch.ones(X.shape).to(X)\n",
    "    for model_index in range(X.shape[-1]):\n",
    "        these_X = X[..., model_index]\n",
    "        direction, value = constraints[model_index]\n",
    "        successful[..., model_index] = (\n",
    "            these_X < value if direction == \"lt\" else these_X > value\n",
    "        )\n",
    "    return successful\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "h1 = ax.contourf(Xplt.cpu(), Yplt.cpu(), Zplt.cpu(), 20, cmap=\"Blues\", alpha=0.6)\n",
    "fig.colorbar(h1)\n",
    "ax.contour(Xplt.cpu(), Yplt.cpu(), Zplt.cpu(), [-10, 0], colors=\"k\")\n",
    "\n",
    "feasible_inds = (\n",
    "    identify_samples_which_satisfy_constraints(Y, constraints)\n",
    "    .prod(dim=-1)\n",
    "    .to(torch.bool)\n",
    ")\n",
    "ax.plot(X[feasible_inds, 0].cpu(), X[feasible_inds, 1].cpu(), \"sg\", label=\"Feasible\")\n",
    "ax.plot(\n",
    "    X[~feasible_inds, 0].cpu(), X[~feasible_inds, 1].cpu(), \"sr\", label=\"Infeasible\"\n",
    ")\n",
    "# ax.scatter(X[:5, 0], X[:5, 1], marker = 'o', s=100, color = 'k')\n",
    "ax.scatter(X.cpu()[:10, 0], X.cpu()[:10, 1], marker = 'o', s=100, color = 'k')\n",
    "ind = 1\n",
    "for i in X[10:]:\n",
    "    plt.text(i[0],i[1],ind, size = 15)\n",
    "    ind += 1\n",
    "ax.legend(loc=[0.7, 0.05])\n",
    "ax.set_title(\"$f_1(x)$\")  # Recall that f1(x) = f2(x)\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "# ax.set_xlim([-0.05, 1.05])\n",
    "# ax.set_ylim([-0.05, 1.05])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fa163922eb0b3709bbb5d8082b2465c9de796dbaacca80cbaa600e7fff3e4fe"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
